# Descente_de_Gradient
Exp√©rimentation des descentes de gradients √† pas fixe et √† pas optimal 
# TP d'Optimisation II : Algorithmes de Descente
**Cours :** INF4127 - Optimisation II
**Universit√© :** Universit√© de Yaound√© 1
**Date :** Novembre 2025

---

## üéØ Objectif du Projet
Ce projet vise √† impl√©menter et analyser le comportement de deux algorithmes d'optimisation sans contraintes sur trois fonctions tests classiques. L'objectif est de comprendre l'influence du **pas de descente** ($\rho$) et de la **g√©om√©trie de la fonction** sur la convergence.

## üìê Rappels Th√©oriques

Nous cherchons √† r√©soudre : $\min_{x \in \mathbb{R}^n} f(x)$.

### 1. Algorithme du Gradient √† Pas Fixe
C'est la m√©thode la plus simple. On se d√©place dans la direction oppos√©e au gradient avec un pas constant $\rho$.
$$x_{k+1} = x_k - \rho \nabla f(x_k)$$
* **Avantage :** Simple √† coder.
* **Inconv√©nient :** Le choix de $\rho$ est critique (trop petit = lent, trop grand = divergence).

### 2. Algorithme du Gradient √† Pas Optimal (Steepest Descent)
√Ä chaque it√©ration, on cherche le meilleur pas $s_k$ qui minimise la fonction dans la direction de descente $d_k = -\nabla f(x_k)$.
$$s_k = \arg \min_{s > 0} f(x_k + s d_k)$$
$$x_{k+1} = x_k + s_k d_k$$
* **Avantage :** Convergence garantie √† chaque √©tape.
* **Inconv√©nient :** Co√ªteux en calcul et peut faire des "zig-zags" dans les vall√©es √©troites.

## üìÇ Structure du D√©p√¥t
Le travail est divis√© en trois analyses ind√©pendantes :

| Fichier | Fonction √âtudi√©e | Particularit√© 
| :--- | :--- | :--- | 
| `1_Rosenbrock.ipynb` | Rosenbrock | Vall√©e √©troite (Conditionnement difficile) 
| `2_Quadratique.ipynb` | Quadratique | Point Selle (Non convexe) 
| `3_Himmelblau.ipynb` | Himmelblau | Multi-modalit√© (4 minima) 

## üöÄ Installation et Ex√©cution
Pour lancer les notebooks, assurez-vous d'avoir les librairies suivantes :
```bash
pip install numpy matplotlib pandas scipy
